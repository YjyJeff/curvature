use super::PartialOrdExt;
use crate::aligned_vec::AlignedVec;
use crate::array::{Array, BooleanArray, PrimitiveArray};
use crate::bitmap::Bitmap;
use crate::compute::logical::and_inplace;
use crate::compute::{IntrinsicSimdType, IntrinsicType};
use crate::utils::roundup_loops;
use num_traits::cast::AsPrimitive;
use std::simd::cmp::{SimdPartialEq, SimdPartialOrd};
use std::simd::{f32x16, f64x8, i16x32, i32x16, i64x8, i8x64, u16x32, u32x16, u64x8, u8x64};
use std::simd::{LaneCount, Mask, MaskElement, SimdElement, SupportedLaneCount};

/// Trait used to bridge the mask type generated by the simd comparison and and the bit mask
/// integer type
pub trait ToBitMask {
    /// Convert self to u64
    fn bitmask(self) -> u64;
}

impl<T, const N: usize> ToBitMask for Mask<T, N>
where
    T: MaskElement,
    LaneCount<N>: SupportedLaneCount,
{
    #[inline]
    fn bitmask(self) -> u64 {
        self.to_bitmask()
    }
}

/// Compare intrinsic simd type
pub trait IntrinsicSimdOrd: IntrinsicSimdType + SimdPartialOrd<Mask = Self::SimdMaskType>
where
    u64: AsPrimitive<Self::BitMaskType>,
{
    /// Type of the bitmask
    type BitMaskType: Copy + 'static;

    /// Mask type generated by the SIMD comparison
    type SimdMaskType: ToBitMask;
}

macro_rules! impl_simd_ord {
    ($({$ty:ty, $simd_ty:ty, $lanes:expr, $bitmask:ty}),+) => {
        $(
            impl IntrinsicSimdOrd for $simd_ty {
                type BitMaskType = $bitmask;
                type SimdMaskType = Mask<<$ty as SimdElement>::Mask, $lanes>;
            }
        )+
    };
}

crate::macros::for_all_intrinsic!(impl_simd_ord);

#[inline(always)]
fn cmp_scalar_<T, F>(
    lhs: &AlignedVec<T>,
    rhs: T,
    dst: &mut [<T::SimdType as IntrinsicSimdOrd>::BitMaskType],
    cmp: F,
) where
    T: IntrinsicType,
    T::SimdType: IntrinsicSimdOrd,
    u64: AsPrimitive<<T::SimdType as IntrinsicSimdOrd>::BitMaskType>,
    F: Fn(T::SimdType, T::SimdType) -> <T::SimdType as IntrinsicSimdOrd>::SimdMaskType,
{
    let rhs = T::SimdType::splat(rhs);
    lhs.as_intrinsic_simd()
        .iter()
        .zip(dst)
        .for_each(|(lhs, dst)| {
            *dst = cmp(*lhs, rhs).bitmask().as_();
        });
}

macro_rules! cmp_scalar_ {
    ($cmp_scalar_macro:ident) => {
        crate::dynamic_func!(
            $cmp_scalar_macro,
            <T>,
            (lhs: &AlignedVec<T>, rhs: T, dst: &mut [<T::SimdType as IntrinsicSimdOrd>::BitMaskType]),
            where
                T: IntrinsicType,
                T::SimdType: IntrinsicSimdOrd,
                u64: AsPrimitive<<T::SimdType as IntrinsicSimdOrd>::BitMaskType>
        );
    };
}

macro_rules! eq_scalar {
    ($lhs:ident, $rhs:ident, $dst:ident) => {
        cmp_scalar_($lhs, $rhs, $dst, |lhs, rhs| lhs.simd_eq(rhs))
    };
}
cmp_scalar_!(eq_scalar);

macro_rules! ne_scalar {
    ($lhs:ident, $rhs:ident, $dst:ident) => {
        cmp_scalar_($lhs, $rhs, $dst, |lhs, rhs| lhs.simd_ne(rhs))
    };
}
cmp_scalar_!(ne_scalar);

macro_rules! gt_scalar {
    ($lhs:ident, $rhs:ident, $dst:ident) => {
        cmp_scalar_($lhs, $rhs, $dst, |lhs, rhs| lhs.simd_gt(rhs))
    };
}
cmp_scalar_!(gt_scalar);

macro_rules! ge_scalar {
    ($lhs:ident, $rhs:ident, $dst:ident) => {
        cmp_scalar_($lhs, $rhs, $dst, |lhs, rhs| lhs.simd_ge(rhs))
    };
}
cmp_scalar_!(ge_scalar);

macro_rules! lt_scalar {
    ($lhs:ident, $rhs:ident, $dst:ident) => {
        cmp_scalar_($lhs, $rhs, $dst, |lhs, rhs| lhs.simd_lt(rhs))
    };
}
cmp_scalar_!(lt_scalar);

macro_rules! le_scalar {
    ($lhs:ident, $rhs:ident, $dst:ident) => {
        cmp_scalar_($lhs, $rhs, $dst, |lhs, rhs| lhs.simd_le(rhs))
    };
}
cmp_scalar_!(le_scalar);

/// # Safety
///
/// - `array`'s data and validity should not reference `dst`'s data and validity. In the computation
/// graph, `array` must be the descendant of `dst`
///
/// - No other arrays that reference the `dst`'s data and validity are accessed! In the
/// computation graph, it will never happens
unsafe fn cmp_scalar<T, F>(
    selection: &mut Bitmap,
    array: &PrimitiveArray<T>,
    scalar: T,
    dst: &mut BooleanArray,
    cmp_func: F,
) where
    T: PartialOrdExt,
    T::SimdType: IntrinsicSimdOrd,
    PrimitiveArray<T>: Array<Element = T>,
    u64: AsPrimitive<<T::SimdType as IntrinsicSimdOrd>::BitMaskType>,
    F: Fn(&AlignedVec<T>, T, &mut [<T::SimdType as IntrinsicSimdOrd>::BitMaskType]),
{
    debug_assert_selection_is_valid!(selection, array);

    and_inplace(selection, array.validity());

    // FIXME: According to selection, determine full or partial
    // Full computation
    {
        cmp_func(
            &array.data,
            scalar,
            std::slice::from_raw_parts_mut(
                dst.data_mut()
                    .mutate()
                    .clear_and_resize(array.len())
                    .as_mut_ptr() as _,
                roundup_loops(array.len(), <T::SimdType as IntrinsicSimdType>::LANES),
            ),
        );
        and_inplace(selection, &dst.data);
    }
}
